\chapter{Track Finding in basf2} \label{chapter-workflow}

After having described the principle of the track finders implemented in basf2 in the previous chapters, the actual changes to the software implementation during this thesis are discussed in this chapter. Before going into detail, the figures of merit of the two track finders showing their advantages and drawbacks are presented in the first section. After a short introduction in the general structure, the newly implemented background hit finder and the performed improvements on the stereo Legendre track finder are shown. Then the newly written \texttt{SegmentTrackCombinerModule} is described. The chapter ends with a section on general tools how to improve the found track candidates and an outlook.

All the plots showing recent results are done with the svn revision 21586. If the implementation before this thesis is compared to the recent results, svn revision number 18262 is used.

\section{FOM of the two track finders}

In the following, the figures of merit of the both track finders -- the local and the global one -- are shown as they were before the works in this thesis. For comparison, the reference implementation trasan is shown. Trasan is the old track finder algorithm developed for the former Belle detector and was adopted to the new Belle~II geometry. It was highly optimized in the run of the experiment but will not be used for the Belle~II experiment, as it is neither build on modern programming principles nor easy to extend by other algorithms and most of the time also poorly documented. However, the figures of merit of trasan will be the levelling staff every new track finder has to be compared with as also done in the following.

Table~\ref{tab-old-implementation-results} shows the overall figures of merit for the two track finders running standalone together with trasan. A detailed analysis can be found below. The table shows the finding and hit efficiency together with the clone and fake rate as described in chapter~\ref{chapter-theory} aggregated over 1000 generic $\PB \APB$-events. Together with these figures of merit, also the same numbers for primaries only are calculated using the \verb+isPrimary+ flag of the MC particles. As therefore a correct matching between tracks from the track finders and the MC particles is needed, it can only be calculated for non-fake tracks. This is why he fake rate is meaningless and not shown in the table. The time per event is measured with the basf2-own performance tools and are of course machine-dependent. All figures presented in this thesis were calculated using the same desktop machine.

As expected, both the Legendre and the automaton track finder can not reach the figures of merit of the reference implementation. The Legendre track finder is in addition to that also much slower than trasan which is mainly due to a non-optimized stereo hit finder that will be discussed in section~\ref{section-stereo}. The automaton track finder has the worst figures of merit when it comes to efficiency. The problems in the implementation lay in the combination of the found segments in the second step (refer to chapter~\ref{chapter-theory}), which fails in most of the cases. As can be seen below, when using the segments alone, the purity and efficiency is very high.

\begin{table}
  \caption{Table showing the figures of merit for the two track finder implementations (global and local) and trasan.}
  \centering
  \begin{tabular}{lccc} \toprule
    & Legendre Track Finder & Automaton Track Finder & Trasan \\ \midrule
    Finding Efficiency & 83.17 & 12.84 & 85.47 \\
    \quad on primaries & 89.94 & 14.20 & 91.39 \\ 
    Hit Efficiency     & 74.03 & 58.29 & 82.19 \\
    \quad on primaries & 78.22 & 59.76 & 87.40 \\ 
    Clone Rate         & 7.69  & 14.02 & 19.72 \\
    \quad on primaries & 4.71  & 26.54 & 17.27 \\ 
    Fake Rate          & 26.82 & 48.17 & 15.72 \\ 
    Time per Event in ms & 635.05 & 32.34 & 441.68 \\ \bottomrule
  \end{tabular}
  \label{tab-old-implementation-results}
\end{table}

\paragraph{Legendre Track Finder}

\paragraph{Local Track Finder}

Standalone legendre + standalone local
Legendre (old!): Finding efficiency, d0 influence, purity
Local: Segment purity, Finding efficiency; Clone + Fake-Rate of both track finder (Local with combined Segments)
Timing.

\todo{pictures: Finding efficiency legendre over pt, d0 influence, purity as histogramm}
\todo{pictures: Timing of legendre + stereo and local}
\todo{pictures: Purity of segments as hist, Finding efficiency over pt + d0}

\section{The \texttt{TrackFindingCDC} Package}
As presented in the section before, the two track finding algorithms have different characteristics. To use the benefits of both, a combined approach is suited best. The proposed workflow as a result of the analysis done in this thesis is presented simplified in figure~\ref{fig-workflow}. The idea is to run both track finders on the same (full) set of hits and combine the resulting tracks/segments afterwards. Another solution is to use only the hits that are not already used by the first track finder in the second one. This proposed approach has an increased computing time in contrast to this solution but has some other advantages: 
\begin{itemize}
 \item If a track is found by both track finders, the probability of it being a fake is very small.
 \item If a track is only found partly by the first track finder, the second algorithm will probably not find the rest as only some hits are unused. When both track finders use the full set of hits, the combined tracks include most of the hits of a track.
 \item Both track finder can be optimized independently. It is even possible to run only one of them to save computing time or for special detector setups (e.g. for cosmic runs) without changing much in the software configuration.
\end{itemize}

\begin{figure}
  \centering
  \begin{tikzpicture}[thick]
    \node[module] (simulation) {Simulation or Experiment};
    \node[module, below=1 of simulation] (background) {Filter Background Hits};
    \draw[vecArrow] (simulation) -- (background) node [midway, anchor=west] {CDC Hits};
    \node[module, below right=1.8 of background] (local) {Local Track Finder};  
    \node[module, below left=1.8 of background] (global) {Global Track Finder};  
    \draw[vecArrow] (background) -- (local) node [midway, auto=false, anchor=south, sloped] {CDC Hits};
    \draw[vecArrow] (background) -- (global) node [midway, auto=false, anchor=south, sloped] {CDC Hits};
    \node[module, below right=1.8 of global] (combiner) {Segment and Track Combiner};  
    \draw[vecArrow] (local) -- (combiner) node [midway, auto=false, anchor=south, sloped] {Segments};
    \draw[vecArrow] (global) -- (combiner) node [midway, auto=false, anchor=south, sloped] {Tracks};
    \node[module, below=1 of combiner] (fitter) {Track Fitter};
    \draw[vecArrow] (combiner) -- (fitter) node [midway, auto=false, anchor=west] {Tracks};
  \end{tikzpicture} 
 \caption[Proposed workflow in the CDC tracking]{The proposed workflow and combination of the two track finders in the CDC detector. The green boxes refer to one or a few modules. The arrows describe parts of the data flow between the modules. For clarity not all necessary parts are shown here.}
 \label{fig-workflow}
\end{figure}

For better interconnectivity between the track finders, they share a large code basis including common data objects and module system in addition to the already given framework by basf2. Combining the code basis of the two track finders was also part of this thesis. The common software design consists e.g.\ of a common singleton (called \texttt{CDCWireHitTopology}) saved in the data store which is responsible for storing the used-flag of the CDC hits and their connection to other objects (like clusters or segments). Also the in- and output of all tracking modules in the CDC is handled by shared code which makes changes in the data flow much easier.

Figure~\ref{fig-workflow2} shows the same workflow as presented im figure~\ref{fig-workflow} (except the simulation and the track fit), but now with the correct module names and the relevant data flow. The \texttt{CDCWireHitTopology} singleton is not parts of the modules but gets used by all modules to check the usage flag and the geometrical information of the wire hits. The modules in the path are described in the following in more detail.

\begin{description}
  \item[Wire\-Hit\-Topology\-Preparer] This module receives the simulated or measured hit information from the CDC detector and connects it with the geometrical information from a database. All hits with configurable flags are saved in the wire hit topology object which is stored as a persistent \texttt{StoreObj} in the data store.
  \item[Segment\-Finder\-CDC\-Facet\-Automaton\-Dev] The module is the first part of the local track finder as described in chapter~\ref{chapter-theory}. As the clusterizer is one part in the cellular automaton used in this algorithm it is -- apart from creating segments -- also used for separating signal and background hits. This filter is described in section~\ref{section-background}. The rest of the module was not developed in this thesis and can be found elsewhere~\cite{oliver}. As all CDC tracking modules communicate over the wire hit topology object, the background flag is propagated to all other modules.
  \item[CDC\-Legendre\-Tracking] Running the axial Legendre track finder with the quad tree search is done in this part as described also in chapter~\ref{chapter}. It uses the full wire hit set (except the background hits). It has also some post-processing steps included directly after the search, which were analyzed and improved in this thesis. \todo{describe?}
  \item[Track\-Quality\-Asserter\-CDC] Although all track finding algorithms are highly optimized there is a need for quality improvement tools for the resulting track candidates. To make this step configurable for later adjustment and to share the code basis between the algorithms, tools for track corrections were developed in this thesis (see section~\ref{section-quality}). They can be used in the other modules directly or via this module which is used in two positions in the path with different configurations.
  \item[Stereo\-Hit\-Finder\-CDC\-Legendre\-Histogramming] After the axial track finder created tracks, they get used in this module to add stereo hits with a quad tree search also. This module was rewritten from scratch in this thesis and is described in section~\ref{section-stereo}.
  \item[Segment\-Track\-Combiner\-Dev] The final step (except for quality improvements) is the combination of the results of the global and local track finder which is handled by this module. It was created in this thesis also and is presented in section~\ref{section-combiner}.
\end{description}

\begin{figure}
  \centering
  \begin{tikzpicture}[thick]
    \node[module, text width=12em] (hits) {{Wire\-Hit\-Topology\-Preparer}};
    \node[module, below=1 of hits, text width=12em] (segment) {{Segment\-Finder\-CDC\-Facet\-Automaton\-Dev}};
    \node[module, below=1 of segment, text width=12em] (axial) {{CDC\-Legendre\-Tracking}};
    \node[module, below=1 of axial, text width=12em] (quality1) {{Track\-Quality\-Asserter\-CDC}};
    \node[module, below=1 of quality1, text width=12em] (stereo) {{Stereo\-Hit\-Finder\-CDC\-Legendre\-Histogramming}};
    \node[module, below=1 of stereo, text width=12em] (combiner) {{Segment\-Track\-Combiner\-Dev}};
    \node[module, below=1 of combiner, text width=12em] (quality2) {{Track\-Quality\-Asserter\-CDC}};
    
    \node[cloud, right=3 of hits] (topo) {{CDCWireHitTopology}};
    
    \draw[vecArrow] (hits) -- (topo) node [midway, anchor=south, sloped, auto=false] {All CDC Hits};
    \draw[vecArrow] (topo) -- (segment) node [midway, anchor=south, sloped, auto=false] {All CDC Hits};
    \draw[vecArrow] (segment.east) -- (topo.south) node [midway, anchor=north, sloped, auto=false] {Filtered CDC Hits};
    \draw[vecArrow] (topo.south) -- (axial.east) node [midway, anchor=north, sloped, auto=false] {Filtered CDC Hits};
    \draw[vecArrow] (axial) -- (quality1) node [midway, anchor=west] {Axial Tracks};
    \draw[vecArrow] (quality1) -- (stereo) node [midway, anchor=west] {Corrected Axial Tracks};
    \draw[vecArrow] (stereo) -- (combiner) node [midway, anchor=west] {Full Tracks};
    \draw[vecArrow] (combiner) -- (quality2) node [midway, anchor=west] {Combined Tracks};
    \draw[vecArrow] (segment.west) to[out=-150, in=150] node [midway, anchor=east] {Segments} (combiner.west) ;
  \end{tikzpicture} 
 \caption[Detailed workflow in the CDC tracking]{The detailed workflow for the track finding procedure for the CDC detector with the correct names of the modules as in the software. The green boxes refer to one modules. The red ellipse is an object on the data store. Some dependencies between the modules are shown as arrows.}
 \label{fig-workflow2}
\end{figure}

\section{The Background Hit Finder} \label{section-background}
The beforementioned track finders are tuned to not pick up background hits into a found track candidate or even form a candidate from background hits only. Nevertheless even a single background hit per track can lead to a reduced momentum resolution or can even cause the fit to fail completely. Together with the reduced combinatorics and therefore reduced computing time when throwing away unusable hits, there is the need to distinguish between signal and background hits even before the tracking starts. This is the purpose of the background hit finder described in this section. The background hit finder is part of the \texttt{SegmentFinderCDCFacetAutomatonDev} module but is described here as a standalone part.

For deciding if a hit is to be used in the track finder, the module uses implemented filters. As these filters are used widely in the whole CDC tracking framework, they will be described here in more detail. To filter out bad hits, not only the information of one hit but the information of the clusters constructed in the local track finder is used. As described in chapter~\ref{chapter-theory} they are created by clusterizing all CDC hits in such a way that one cluster includes the maximum number of connected hits. Two hits are called connected if there is no non-fired wire geometrically between them. Because of the modular framework and the reusable \texttt{CDCWireHitTopology} it is possible to run the local track finder with the clusterizer before the legendre track finder and propagate the background information to the following tracking algorithms. 

Figure~\ref{fig-clusters} in chapter~\ref{chapter-theory} shows an event display in the $r$-$\phi$-plane with the clusters drawn in different colors. Figure~\ref{fig-cluster-hit-purity} shows the hit purity of clusters from typical events. The hit purity is computed as the ratio between the number of hits in a cluster which belong to a signal track divided by the number of all hits (including the ones coming from background only). As can be seen clearly in this histogram a cluster is either full of background or full of signal hits - for that throwing away a background cluster does not involve deleting hits that are needed for signal tracks. However using clusters instead of single hits for deciding the signalness of the hits has the advantage of more information about the neighborhood of the hits.

\begin{figure}
  \centering
  \includegraphics[width=0.7\linewidth]{figures/workflow/cluster_purity.png}
  \caption{Histogram of the hit purities (ratio between signal hits to all hits in a cluster) of the found clusters by the clusterizer as part of the local track finder algorithm. As the clusters are either purely signal or purely background, they can be kept or thrown away as whole objects.}
  \label{fig-cluster-hit-purity}
\end{figure}

In every event, each formed cluster is passed to the chosen filter with the chosen filter parameters to decide whether to use the hits in this clusters for track finding or not. This filter can return every number as a result including the C++ std::nan, which is used as the value for indicating that the cluster should not be used further for track finding as it is marked as background by the filter.

The filter design is very general and leaves the possibility open to compare many different filters. For the background filter as well as for many other used filter in the CDC tracking, five filters are implemented which can be selected by using module parameters. These filters are:

\begin{description}
  \item[BaseFilter] is a filter that neglects every item. It is the parent base object for every other filter and is rarely used in production.
  \item[AllFilter] is the counterpart of the BaseFilter and accepts all items. It can be used for testing purposes to make a filter fully transparent.
  \item[SimpleFilter] is a filter based on self-implemented cuts. The variables to cut depend on the planned usage of the filter and need to be implemented by the user.
  \item[TMVAFilter] is based on a trained boosted decision tree. It passes the variables defined in a so called VarSet to the BDT and returns a result between 0 and 1. If the result is lower than a certain cut definable on runtime, the TMVAFilter returns std::nan otherwise the result of the BDT. The variables can be freely defined by the user. In most of the cases the separating force of a well-trained TMVAFilter exceeds the one of a simple filter.
  \item[RecordingFilter] returns only a single constant defined on runtime independent on the input and can therefore not directly be called a filter. Instead, its purpose is to write the same variables as the TMVAFilter uses together with a truth information of every incoming item into a ROOT \texttt{TNTuple}. The output file with this \texttt{TNTuple} can later be used to train a BDT to categorize according to the truth information. This filter can of course only be used on simulated data.
  \item[MCFilter] filters the items according to the truth information compiled with the MC information that is also used in the recording filter.
\end{description}

As an example for the mentioned VarSet, the variables for distinguishing the background clusters from the signal clusters is described in table~\ref{tab-varset-cluster}. The truth information used for the BDT training is also described there. It is not always obvious why the described variables can help to separate between background and signal cluster. This is why one typical signal and one background cluster is shown in figure~\ref{fig-cluster-versus}. As can be seen in the figure, background clusters have a different shape as signal clusters. Particles creating signal hits pass almost straight through the wires leaving behind a defined long-shaped trace of hits whereas background hits get created in a single spot forming more circular-shaped clusters (if they have more than one hit at all). This shape-information is used in the number of neighbors as well as in the averaged distance to the superlayer center. The latter is zero for most of the signal clusters as the particles go through the whole superlayer whereas background hits do not span the whole section. As the background clusters are made of hits coming from stochastically distributed processes, the parameters of the hits like time information (which gets transformed to the drift length) or deposited energy (which is encoded in the ADC count) are probably randomly distributed. This is why the first two momenta of the distributions (mean and variance) are also taken into account.

\begin{figure}
  \centering
  \begin{minipage}{0.58\linewidth}
    \centering
    \includegraphics[scale=0.8]{figures/workflow/cluster_display_background.pdf}
  \end{minipage}
  \begin{minipage}{0.4\linewidth}
    \centering
    \includegraphics[scale=0.8]{figures/workflow/cluster_display_signal.pdf}
  \end{minipage}
  \caption{Detailed view of a full event display (cf. figure \ref{fig-} in chapter~\ref{chapter-theory}) with found clusters colored differently for better visualization. The left subfigure shows background clusters, whereas the right one shows clusters with hits from simulated signal particles. The differences in the described variables like shape, number or distance can be seen.}
  \label{fig-cluster-versus}
\end{figure}


\begin{table}
  \centering
  \begin{tabular}{p{0.35\linewidth}p{0.6\linewidth}} \toprule
   Variable Name & Description \\ \midrule
   \verb+is_stereo+ & Boolean variable if the cluster is in a stereo superlayer or not. Has no separating force by itself but only in combination with other variables. \\ \midrule 
   \verb+superlayer_id+ & The index of the superlayer counted from the most inner superlayer outwards. As the \verb+is_stereo+ variable, is should is used in combination with other variables.\\ \midrule 
   
   \verb+size+ & The number of hits combined to a cluster. This variable by itself has the best separation power among all variables, as background hits occur most likely as disconnected hits whereas signal hits are connected by the track they form.  \\ \midrule 
   
   \verb+total_number_of_neighbors+ & \multirow{2}{*}[-1.5pt]{\begin{minipage}{\linewidth} The number of neighbors for on hit is the count of hits in the direct surrounding of a wire hit and can therefore be at most six (as can be seen in figure~\ref{fig-sense-wires} in chapter~\ref{chapter-ex}). The sum of all neighborhood numbers for all hits in the cluster and the total number divided by the number of wire hits are used. \end{minipage}} \\[5ex] \cmidrule{1-1}
   \verb+mean_number_of_neighbors+ & \\[5ex] \midrule 
   
   \verb+total_drift_length+ & \multirow{3}{*}[-1.5pt]{\begin{minipage}{\linewidth} The drift length is a function of the measured time delta between the collision and the moment the drifting electrons touch the sense wires. The sum over all hits in a cluster, the averaged drift length and the variance is used. \end{minipage}} \\[1ex] \cmidrule{1-1}
   \verb+mean_drift_length+ & \\[1ex] \cmidrule{1-1}
   \verb+variance_drift_length+ & \\[1ex] \midrule 
   
   \verb+total_inner_distance+ & \multirow{3}{*}[-1.5pt]{\begin{minipage}{\linewidth} The inner distance is the geometrical difference between the wire position in the $r$--$\phi$ plane and the interaction point. For stereo hits, the position for $z = 0$ is chosen. \end{minipage}} \\[1ex] \cmidrule{1-1}
   \verb+mean_inner_distance+ & \\[1ex] \midrule
   \verb+distance_to_+ \verb+superlayer_center+ & Instead of calculating the distance to the interaction point, the averaged radial distance from the wire hits to the radius of the center of the superlayer the cluster lays in is calculated. \\ \midrule 
   
   \verb+total_adc_count+ & \multirow{3}{*}[-1pt]{\begin{minipage}{\linewidth} Together with the time delta of the incoming drifting electrons each sense wire measures also the number of electrons that were ionized in the drift cells. The ADC count is the digitized value of this number. \end{minipage}} \\ \cmidrule{1-1}
   \verb+mean_adc_count+ & \\ \cmidrule{1-1}
   \verb+variance_adc_count+ & \\ \midrule
   \verb+truth+ & A cluster is counted as signal if more than 80 \% of the contained hits belong to a simulated MC particle (and not to background). As the number of hits in one cluster is mostly very small, this implies that all hits must be from a signal track for most of the cases. \\ \bottomrule
  \end{tabular}

  \caption{The variables used for the classifier to separate background from signal clusters (and hits). Additional information on some of the variables can be found in the text.}
  \label{tab-varset-cluster}
\end{table}

After the recording and the training procedure (which is handled by Python scripts) the TMVAFilter with the mentioned VarSet can be used. Figure~\ref{fig-result-background-hit-finder} shows the ROC curve (left side) and the distribution of the BDT result for the training and the testing set of clusters. As can be seen, the BDT has a very good separation power between background and signal clusters. As the output distributions for testing and training data set is very similar, there is also no overtraining visible.

\begin{figure}
  \centering
  \includegraphics[width=0.48\linewidth]{figures/workflow/background_hit_finder_roc.png}
  \includegraphics[width=0.48\linewidth]{figures/workflow/background_hit_finder_overtraining.png}
  \caption{The left plot shows the background rejection over the signal efficiency (so-called ROC curve) for the trained BDT to distinguish between signal and background clusters. Also shown is the ROC curve recalculated using the hits in each cluster. As the number of hits in each cluster is not constant, the two curves differ. The right histogram describes the distribution of the output variable of the BDT for signal and background clusters in the testing (cross markers) and the training (bars) data set. No overtraining is visible as the distributions match perfectly well.}
  \label{fig-result-background-hit-finder}
\end{figure}

By applying a cut on the BDT output the background hits can be marked and do net get used in the following tracking routines. How this cut is chosen depends on the desired optimization. A lower cut can include many background hits but does not throw away signal hits keeping the fake rate as well as the hit efficiency high. A harsher cut however can increase the finding efficiency as the combinatorics are reduced and the probability to collect background hits is reduced. Figure~\ref{fig-result-background-hit-finder2} shows the figures of merit of the axial Legendre track finder alone with different cuts on the BDT output. Especially the fake rate can be strongly reduced by using the background hit finder. Figure~\ref{fig-hits-numbers} shows the number of signal/background hits and clusters with different cut values. As the number of signal clusters/hits stays equal but the number of background clusters/hit is reduced with higher cut values, the combinatorics are reduced as well. This can be seen in figure~\ref{fig-performance-clusters}, where the computation time of the local track finder algorithm (which is in particular sensitive to the combinatorics) is shown.

\begin{figure}
  \centering
  \includegraphics[width=0.48\linewidth]{figures/workflow/background_hit_finder_efficiency.png}
  \includegraphics[width=0.48\linewidth]{figures/workflow/background_hit_finder_rate.png}
  \caption{Figures of merit of the Legendre track finder (without combination with the automaton track finder and without final track quality corrections) for different cut values on the background hit finder BDT. With the background hit finder, the rate of fakes could be reduced drastically. The finding efficiency and the clone rate stay almost the same. When the cut on the BDT output is to hard, the hit efficiency is decreased.}
  \label{fig-result-background-hit-finder2}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.48\linewidth]{figures/workflow/number_of_clusters.png}
  \includegraphics[width=0.48\linewidth]{figures/workflow/number_of_hits.png}
  \caption{Number of signal/background clusters/hits after a cut on the BDT output. The number of signal hits stays more or less the same for the lower cut value region whereas the number of background hits decreases.}
  \label{fig-hits-numbers}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.6\linewidth]{figures/workflow/background_hit_finder_performance.png}
  \caption{Averaged computing time per event of the two track finder algorithms (local and global) in sum with the already implemented changes as described in the following sections for different cut values on the BDT output. Except for statistical fluctuations, the computing time decreases with harder cut, as the number of hits gets smaller.}
  \label{fig-performance-clusters}
\end{figure}

\todo{describe axial or not?}
% \section{Improvements on the Axial Legendre Track Finder}
% Is already very good and advanced. 

% \subsection{The class \texttt{QuadTreeProcessorTemplate}}
% \subsection{Postprocessing after the track finding}
% \subsection{Results}
% Timing

\section{Improvements on the Stereo Legendre Hit Finder} \label{section-stereo}

The previous implementation of the stereo hit finder yielded good finding and hit efficiencies with an acceptable fake and clone rate. However, the module was partly built with legacy code and had a very bad computing performance as the processing time of the stereo finder was about 20 times higher than the axial finder. Instead of refactoring the module, it was rebuilt from scratch in this thesis using the common code basis for all track finder algorithms in the CDC and the same quad tree structure as the axial hit finder. Additionally, a refined stereo position calculation was implemented using the already provided methods from the framework and a second coordinate in the quad tree structure was introduced. Before, the stereo hit finder used only the $\lambda$ angle for a histogramming method whereas in the new module the the slope in the $s$--$z$-plane and $z_0$ are used as described in chapter~\ref{chapter-theory}. Because of an introduced caching method, the computation could be speed up by a factor of approximately 100. The algorithm is described in the next paragraph whereas the results in comparison to the old implementation are shown after that.

\subsection{Implemented algorithm}

The stereo hit finding algorithm consists of three stages, which are repeated for every found axial track. In between the iterations for every track, the found hits are marked to not use them twice. The steps are in detail:
\begin{zlist}
  \item \textit{Fill a vector of hits and pass them to the quad tree.} All non-used stereo hits from the wire hit topology (except background hits) are used to create a vector of reconstructed hits. These hits have a cached travel distance $s$ and a $z$ position, that is calculated using the track. It is chosen in such a way, that the trajectory in the $r$--$\phi$-plane touches the drift circle of the wire hit perfectly. As there are inherently two possible orientations of the hit -- left or right -- there are two items in the vector for each wire hit. A check if the travel distance is negative (which happens for hits laying on the other side of the CDC than the track) and if the reconstructed $z$ value is outside of the CDC boundaries is applied. 
  \item \textit{Quad tree search for the best candidate.} The $\tan \lambda$ and the $z_0$ values of the passed right--left-items in the vector are used in a quad tree search. For this, a line in the $s$--$z$-space is constructed with the slope $\tan \lambda$ going through $(0, z_0)$. An item belongs to a quad tree bin if and only if this line goes through the bin borders exactly twice (which can easily be checked using the mean value theorem). As only one single axial track is used for reference at a time, only the highest bin in the quad tree search is used. If both the right and left hypothesis of a hit are in the resulting bin, the one orientation laying more near to the trajectory is used.
  \item \textit{Add the found hits to the axial track and clear the quad tree.} After the search is finished, the found stereo hits are added to the axial hits and the whole track hits are sorted correctly. For this, a line fit is performed to extract the $z$-information of the trajectory. In the end, the quad tree is cleared and prepared for the next track or event.
\end{zlist}

Both quad tree algorithm implementations currently available in the framework are used in the module and can be switched easily by module parameters. As they both use the same principles, only the results of one implementation are shown. Additionally, as base items for the vector passed to the quad tree not only stereo wire hits but also stereo segments coming from the local track finder can be used. Instead of checking if the line in the $s$--$z$-plane intersects with the quad tree bin for a single hit, all hits in the segment are checked for. If more than 70 \% of the hits in a segment intersect with a quad tree bin, the whole segment is counted in in this bin with the number of intersecting hits as a weight. Using segments instead of hits has some benefits: as the hit purity of the segments is rather high, adding whole segments instead of single hits can improve the hit efficiency and purity of the stereo tracks. Also, as the number of segments is much smaller than the number of hits, the combinatorics in the quad tree search is reduced resulting in a smaller quad tree level and a smaller computing time. The drawback, however, is that single wrong or wrongly reconstructed\footnote{As the $z$-position reconstruction depends heavily on the trajectory parameters in the $r$--$\phi$-plane, the calculation can give wrong results when these parameters are off because of energy loss or low hit purity of the axial hist.} hits can influence the segment quite much. The problem is, that if a whole segment is lost because of a few wrong hits, the hit efficiency is reduced.

\subsection{Results}

The two operation modes in the new stereo hit finder module (with bare wire hits or with segments) can be compared to the old stereo hit finder implementation and also with the reference implementation trasan from Belle. The figures of merit and the computation time per event are shown in table~\ref{tab-stereo-results} together with the finding and hit efficiency as a function of $p_T$ in figure~\ref{fig-stereo-results}. 

\begin{table}
  \caption{Table showing the figures of merit for the two new implementations (Hits and Segments), the old implementation and trasan. As trasan is responsible for adding axial and stereo hits, the computing time for stereo only can not be deduced.}
  \centering
  \begin{tabular}{lcccc} \toprule
    & Hits & Segments & Old Implementation & Trasan \\ \midrule
    Finding Efficiency & 86.00 & 87.38 & 83.17 & 85.47 \\
    \quad on primaries & 91.59 & 92.55 & 89.94 & 91.39 \\ 
    Hit Efficiency     & 84.62 & 80.05 & 74.03 & 82.19 \\
    \quad on primaries & 90.20 & 85.50 & 78.22 & 87.40 \\ 
    Clone Rate         & 15.42 & 15.79 & 7.69  & 19.72 \\
    \quad on primaries & 7.29  & 7.63  & 4.71  & 17.27 \\ 
    Fake Rate          & 17.76 & 16.28 & 26.82 & 15.72 \\ 
    Time per Event in ms & 65.20 & 66.67 & 635.05 & 441.68 \\ 
    \quad only Stereo Finder & 2.94 & 4.18 & 578.76 & - \\ \bottomrule
  \end{tabular}
  \label{tab-stereo-results}
\end{table}

\begin{figure}
  \includegraphics[width=0.48\linewidth]{figures/workflow/stereo_finding_efficiency.png}
  \includegraphics[width=0.48\linewidth]{figures/workflow/stereo_hit_efficiency.png}
  \caption{Finding (left) and hit (right) efficiency of the four tracking algorithms with axial and stereo finding. Although the two new implementation look worse than the reference implementation trasan, it must be noticed that in the lower $p_T$ region there are much more particles present leading to a higher weight on these tracks.}
  \label{fig-stereo-results}
\end{figure}

As can be seen, the figures of merit are enhanced by the newly written module and are now comparable or better than the reference implementation trasan. This is mainly because of the additional coordinate in the quad tree which handles track not coming from the interaction point. The computing time is much smaller which is mainly because of caching and optimized calculation procedures, e.g.\ by changing the coordinates in the Legendre space from $\lambda$ which needed heavy trigonometrical calculations to the much easier to calculate $s$--$z$-slope. Please keep in mind, that also some implementation details of the axial Legendre track finder have changed between the old and the new implementation.

The both implementations - using bare hits or segments in the quad tree - share most of their properties like a small execution time and a high finding efficiency with rather low fake and clone rate. As described in the previous sub section, the approach with using segments has a lower fake rate (because the probability of adding wrong hits is rather low) and also a higher finding efficiency, but also a lower hit efficiency as when using bare wire hits in the quad tree. The execution time per event is surprisingly higher when using segments although the depth of the quad tree is reduced. One reason could be that the algorithm to check whether a hit belongs to a quad tree bin or not is more complicated than in the hit approach can because of the high number of iterations in the quad tree search, this can slow down the whole track finder. Further analysis is needed to optimize this module.

\todo{Inhalt: Ist RL-drin? Kann man segment noch verbessern mit anderen parametern? Soll ich die Parameter beschreiben und eine Grid-Search durchführen?}

\section{The \texttt{SegmentTrackCombinerModule}} \label{section-combiner}

To increase the hit efficiency, the segments found by the local track finder and the axial and stereo tracks from the global track finder can be combined. This unifies the benefits of both track finder as the local track finder itself can not handle the combination of whole tracks well whereas the global track finder easily looses some hits of a track. Merging thw two track finders has however also another benefit: if all possible segments and tracks combinations are made, the remaining segments can be used to form new tracks and can help to fill the phasespace gaps of the Legendre track finder. Lastly, tracks without any matching segments are most likely fake tracks and can be deleted. 

\subsection{Principle of the Segment Track Combiner}

The matching procedure between tracks and segments is a multi-step process including up to seven filters. In the default configuration, most of them are turned off because of their huge influence on the finding efficiency. The used filters are still under development and further analyses are needed. In the following tracks refers to the result of the global track finder whereas segments describes the output of the local track finder. The algorithm consists of the following steps:
\begin{zlist}
 \item Creation of a fast segment and track lookup.
 \item First matching of segment and tracks that share one or more hits. \label{list-start}
 \item Deletion of fake segments.
 \item Flagging of segments belonging to particles that probably can not be found by the global track finder.
 \item Matching of the remaining segments with the tracks or among themselves and then with the tracks. \label{list-second}
 \item Filtering of fake tracks in the made combinations.  \label{list-end}
 \item Cleanup of the lookup cache.
\end{zlist}

The steps (\ref{list-start}) - (\ref{list-end}) include one or more filters which consist mostly of a trained BDT as the one described for the background hit finder. Some steps are described in the following in more detail.

\paragraph{First Matching (step \ref{list-start}}

\paragraph{(step \ref{list-second})}

\paragraph{Filtering of the tracks (step \ref{list-end})}
To reduce the fake rate further before giving the tracks to the track fitter and the physics analysis packages, a trained BDT can filter out tracks that are most likely to be fakes. The BDT is trained using the fake output of the MC matching routine as the background flag. To decide whether a track is a fake it uses several input variables compiled from the tracks trajectory and hit content as $p_T$ and $\tan \lambda$ or the first two moments of the drift length or ADC count distributions among the hits. It also calculates a value to measure how ``empty'' the track is, i.e.\ how many hits are probably missing as a track with more missing hits is typically a fake. Figure~\ref{fig-track-filter-roc} shows the ROC curve of the trained BDT. As it can be seen, the performance of the filter is rather bad compared to the other trained filters in this thesis. Figure~\ref{fig-track-filter-result} shows the figures of merit as a function of the BDT cut value on this filter only and confirms this conclusion as it has a high inpact on the finding efficiency. However, in situation where a very low fake rate is necessary, this filter can be used.

\subsection{Results}

\section{The \texttt{Track\-Quality\-Asserter\-CDC\-Module} and the \texttt{Track\-Quality\-Tools}}  \label{section-quality}

The common code basis in the CDC track finding package make it possible to include common correction tools for the resulting tracks into the software framework. The \texttt{Track\-Quality\-Tools} singleton offers functions for applying typical corrections on the track candidates like removing of hits or normalizing the trajectory information to a defined format (e.g.\ the start point of the circular trajectory should lay on the position of the first hit). The different implemented methods are described later.

These tools can be used in the track finder modules directly (instead of implementing the same algorithms again), but are also used standalone (with the module \texttt{Track\-Quality\-Asserter\-CDC\-Module}) after the whole track finding procedure. The reason is not only to reduce the number of wrong hits per track and the fake rate, but also to prepare the tracks for the next step, the track fitting. The fitting algorithm needs a defined format for the trajectory seed parameters (the position seed must lay near the first hit in the track and the momentum seed must be defined at this point). Additionally, the fitting algorithm as it is currently implemented in the framework is very sensible to discrepancies in the track candidates like long segments of missing hits or wrongly added hits because of decay-in-flight particles or high energy loss. The fit can get biased into a wrong direction in the first iteration leading to an increased number of steps and because of more needed material lookups a higher computing time. As the maximal number of iteration per track is limited to improve performance, the fit can fail completely reducing the finding efficiency when taking into account only fitted tracks to about 60 \% (compared to the values of about 90 \% before the fit). To increase the number of successful fits, the tracks can be edited to be suited for fitting including the clipping of hits which reduces the hit efficiency. As only the tracking parameters at the front of the track are important for physics analysis, cutting away parts of the track does not harm the fit resolution directly. The hits however can be used in $\mathrm d E/\mathrm d x$ analysis for particle identification and it may be a good idea to keep the relation to the track and readd them to the fitted results to increase the hit efficiency again. 

Another important step is the removal of axial only tracks which can not be fitted by the currently implemented track fitting procedures as there is no direct $z$ information present in the tracks.\footnote{Tracks without stereo hits have nevertheless a $z$ information in them as the energy loss depends in the travel length in the material which in turn is related to the $\theta$ angle of the track. \cite{martin}} As most of these tracks have no stereo hits because the energy is too low to reach into the first stereo superlayer and not because the track finder algorithms have not found the hits there is no possibility to add stereo information to the tracks in the CDC only. For a different approach -- combining the CDC track candidates with the VXD results -- are shown in section~\ref{section-outlook}.

After running the \texttt{Track\-Quality\-Asserter\-CDC} module before fitting the finding efficiency when only taking into account fitted tracks could be increased but does not reach the finding efficiency before the fit -- so the fitting rate is not 100 \% still. The finding and hit efficiency together with the other figures of merit are shown in table~\ref{tab-results-after-fitting} before and after the fit with and without the quality module. Also, the computing time of the fitting module could be reduced from \todo{numbers}. Once the external fitting package is improved to handle these challenging cases also, some of the corrections can be turned off by changing the steering file parameters of the module.

\begin{table}
  \todo{Results after fitting.}
  \caption{}
  \label{tab-results-after-fitting}
\end{table}

Some of the methods implemented in the \texttt{TrackQualityTools} are described with full details in the following:
\begin{description}
 \item[\texttt{normalizeHitsAndResetTrajectory}] To have a defined starting point in each of the modules and especially for the fitting routines, the trajectory is shifted to start near the first track hit. This hit is determined to be the most inner hit of the track and has an arc length of zero. All other hits have positive arc lengths and are sorted according tho these values. The trajectory for curling tracks is reversed if the number of hits on the ingoing arm is grater than on the outgoing one.
 \item[\texttt{removeHitsAfterLayerBreak} and \texttt{removeArcLength2DHoles}]  As the fitting procedure can not handle tracks with large segments of not found hits very well it is feasible to have routines which cut tracks before those holes. This can be done either by looking on the arc length or the geometrical distance between successive hits in the tracks. An additional method can cope with multiple ``breaks'' in the track and chooses the longest coherent track part.
 \item[\texttt{removeHitsOnTheWrongSide}] The Legendre track finding algorithm has some issues with non-curling back-to-back tracks or at least hits. The problem is described in figure~\ref{fig-b2btrack}. For non-curling tracks these added hits can easily be spotted by looking onto the arc length values which is negative for these hits.
 \item[\texttt{splitSecondHalfOfTrack}] As the reference implementation trasan, which was used successfully in combination with the fitting algorithms currently implemented, splits every curling track on the apogee point, this feature is also implemented in the track quality tools.
\end{description}
In addition, there are methods to remove tracks with a small number of hits or which are localized to a single super layer and other methods which are not described here.

\section{Further Approaches} \label{section-outlook}
\subsection{Quad tree search with segments}
Benefits: Purity high, more or less no reassignment needed. RL-information already present, lower combinatorics = more then one axis possible?

More than one possibility to do that: axial + stereo or only stereo or only axial, when should a segment belong to this legendre bin?

\subsection{VXD-CDC-Merger before fitting}
Benefits: Possible better efficiency (no loss because of fit). Faster, less fakes, better finding efficiency (because a track is kept which would be maybe deleted by a quality tool)

Ho to do it: BDT with trained variables. First results.