\chapter{Track Finder Theory and Multivariate Classification}

Before explaining the implemented changes to the track finder for the Belle II experiment in more detail, the principles of the Belle II software framework are explained briefly. More information can be found elsewere \cite{tdr}. Afterwards the working principles of the already implemented track finders are illustrated and common figures of merit for all track finders are explained and discussed. As some parts of the track finders rely on the usage of multivariate classification the main principle of boosted decision trees can be found in the end of the chapter.

\section{The Belle II Analysis Framework (\texttt{basf2})}

For simulation, data acquisition, data processing and analysis of the Belle II experiment the Belle Analysis Software Framework 2 (\texttt{basf2}) is used. Although - guided by its name - it seems to be build on top of the old software framework used for the Belle experiment it is a complete rewrite of the software using modern programming principles in the coding languages C++ \cite{cpp} and Python 2.7 \cite{python}. Together with external programming libraries like ROOT \cite{root} or EvtGen \cite{evtgen} this framework builds the base for every software written for the experiment.

The software is divided into several packages - each serving a distinct purpose or summarizing code for a single detector. Examples for the packages are CDC, SVD or the tracking packaged which is described in more detail in later chapters.

Each usage of the Belle Analysis Software Framework - if it is either a simulation, a reconstruction or an analysis - consists of processing one or more so called \emph{paths} created with \emph{modules}. These modules perform a dedicated small task like simulating the hard scattering event (the so called \texttt{EvtGen} module), writing out data to a root file (the module is called \texttt{RootOutput}) or performing a track reconstruction (for example with the module \texttt{TrackFinderCDCAutomaton}). The presence, the order and the parameters of the modules are determined in \emph{steering files} written with python. The modules itself can be written in C++ or python. 

In these steering files a path is created, filled and passed to the framework which handles loading the corresponding C++ libraries and calling the modules for every event that should be processed. An example of a small steering file for track finding can be found in listing \ref{lis-steering-file}. Caused by this extremely modular structure not only parallel processing but also debugging of intermediate steps can be performed much easier.

Because many modules need the data produced by other modules before there is a need for intermodular communication. This communication is performed within the framework with the help of the data store. This class is a wrapper around a collection of named \texttt{TClonesArrays} from the ROOT library \cite{tclonesarray} which can store lists of instances of nearly arbitrary C++ classes. It is used widely in the framework to store all sorts of things like the hit information produced by the particles in the simulation or the found tracks after the track finding modules. The modules have read and write access to every so called store array in the data store. A visualization of the data flow between the modules created with the steering file in listing \ref{lis-steering-file} can be found in figure \ref{fig-viz-datastore}. The data store can be written to or read from disk using ROOTs own serialization mechanism together with data member dictionaries for the C++ classes created by the C++ interpreter of ROOT called CINT \cite{cint}.

\begin{listing}
 \begin{lstlisting}[language=Python]
  # Import the needed basf2 package
  import basf2

  # Create a basf2 path
  path = basf2.create_path()

  # Add an inpout module to read from the data.root file
  path.add_module("RootInput", inputFileName="data.root")
  
  # Connect the gearbox which is needed for setting the standard parameters
  path.add_module("Gearbox")

  # Add the legendre track finder
  path.add_module("CDCLegendreTracking", WriteGFTrackCands=False)
  # Add the stereo legendre finder
  path.add_module("StereoHitFinderCDCLegendreHistogramming",
                  SkipHitsPreparation=True,
                  TracksStoreObjNameIsInput=True,
                  WriteGFTrackCands=True)
  
  # Save the data store to a root file
  path.add_module("RootOutput")

  # Process the path
  basf2.process(path)

 \end{lstlisting}
 \caption{Python steering file to create a typical basf2 path. After loading the needed python libraries the path is created and filled with the modules. In the end this path is processed and for each event the modules are executed in the given order and with their given parameters. For more information on the used modules see their documentations.}
 \label{lis-steering-file}
\end{listing}


\begin{figure}
 \centering
 \includegraphics[width=\linewidth]{figures/theory/dataflow.png}
 \caption{The visualization of the intermodular communication while processing the path implemented with the steering file described in listing \ref{lis-steering-file}. The green boxes are entries in the datastore. The ellipses are the processed modules. The red and blue arrows depict input and output to the modules.}
 \label{fig-viz-datastore}
\end{figure}


\section{Working Principle of the implemented CDC Track Finder in \texttt{basf2}}

One part of this work was the improvement and further development of the track finder modules for the CDC tracking detector. Therefore the working principles of the two track finder for this detector are described here briefly. For more information on the first track finder - the legendre track finder - see \cite{kronenbitter}. More information on the second described track finder - the automaton track finder - can be found in \cite{oliver}.

The general purpose of a track finder algorithm is to partition all measured wire hits into exclusive sets of hits that may come from the same charged particle passing through the detector. It does so by using several assumptions on the charged particle producing the wire hits like the form of their trajectory and therefore the possible patterns of the hits. After fitting a mathematical model of a trajectory to these hits one can gain information on the momentum or the vertex position of this particle. This part is called track fitting and is described in a later section. These sets of hits possibly belonging to a charged particle are in the following called track or track candidate.

The reason to have two track finders for the CDC is their different ansatz. The legendre track finder is a so called global track finder whereas the automaton track finder is a local one. A global algorithm uses the information of all wire hits simultaneously. The legendre track finder does this by applying a mathematical transformation to the wire positions which should in principle project all hits belonging to the same track onto the same coordinates. A local algorithm however tries to use neighboring wire hits to construct clusters of hits. These clusters are then enlarged by using neighborhood relations again until a full track can be found. In the following these two principles are described in more detail.

\subsection{The Legendre Track Finder}
The principle of using the legendre transformation for tracking algorithms in high energy particle experiments was first described by Alexopoulos \cite{legendre}. It uses an extended version of the hough transformation introduced by Paul V.C. Hough in 1962 \cite{hough}. The algorithm uses the fact that each trajectory in the $r$-$\phi$-plane of the detector can be described by a circle - assuming no energy loss - because of the applied magnetic field. In a first approximation one can also assume that each particles comes from the interaction point which is valid for the bigger part of the decay products. Therefore the trajectory in the $r$-$\phi$ direction can be described by two parameters: the radius $R$ of the circle and the angle $\theta$ between an arbitrary but fixed axis and the tangent to the circle at the interaction point.\footnote{When dropping the last assumption of tracks coming from the origin one has to introduce another parameter - often called $d_0$ - which describes the minimal distance from the circle to the origin in the $r$-$\phi$ plane. It is easy to generalize the described algorithm to these three dimensions. In the moment this third dimension is however not implemented in the tracking software.}

Simplified, the idea is to calculate each trajectory that could have possibly created one of the axial hits and draw them all in a 2d histogram with the trajectory parameters $R$ and $\theta$ as the coordinate axes. As there are only a small number of correct trajectory which are however responsible for a great number of hits there is a small parameter set which appears very often in the histogram. These parameters can then be used to create tracks.

For applying this algorithm the $x$ and $y$ coordinate pair of every axial wire hit together with the drift length $d$ is transformed by the function
\begin{align*} x' = \frac{2x}{x^2 + y^2 - d^2} \qquad y' = \frac{2y}{x^2 + y^2 - d^2}  \qquad d' = \frac{2R}{x^2 + y^2 - d^2} \end{align*}
$$R = x' \cos(\theta) + y' \sin(\theta) \pm d'$$
into the legendre space as it can be seen in figure \ref{fig-legendre-explained}. In the first step of the transformation, the wire hits are transformed to the inverted plane. With this transformation each circular trajectory through the interaction point is mapped onto a line. The two trajectory parameters $R$ and $\theta$ are now functions of the slope and the axis interception of this line. 

After that each drift circle is transformed into a pair of sinusoidal functions - also called sinograms. This function is constructed in this way to use the fact that each trajectory of a charged particle responsible for a wire hit must touch the drift circle tangentially. Each point on the constructed sinusoidal functions correspond to one possible trajectory of a particle which could have created such a hit. There are two sinusoidal functions because the osculation point can be on the far or the near side of the drift circle - the trajectory circle can circumscribe the drift circle or not.

\begin{figure}
 \centering
 \includegraphics[scale=0.2]{figures/theory/legendre_1.png}
 \includegraphics[scale=0.2]{figures/theory/legendre_2.png}
 \includegraphics[scale=0.2]{figures/theory/legendre_3.png}
 \caption{Transformation of some wire hits belonging to the same charge particle (left side) to the inverted plane (middle) and to the legendre space with the sinusoidal functions (right). As described in the text, the circular trajectory is first transformed into lines and then into intersecting sinograms. Each sinogram includes all possible trajectory parameters which would have touched the hit from which the function was created. The intersection corresponds to the parameters of the trajectory and are marked with a red circle. For better visibility only the wrong half of the sinograms is colored gray.}
 \label{fig-legendre-explained}
\end{figure}

\todo{picture?}
With using the information of a single hit one ends up with an infinite number of trajectory hypothesis. But as a charged particle passed many drift cells until it leaves the CDC detector - in some cases up to 100 hits - several wire hits are created with the same trajectory parameters. As these same parameters correspond to the same point in the legendre space, the sinusoidal functions of the wire hits intersect in this point as it can also be seen in figure \ref{fig-legendre-explained}. The task of the legendre algorithm is now to do the transformation of the hit coordinates and find those intersections.

Imperfections due to energy loss and material effects make the sinusoidal function not intersect in one single point but rather in a smeared area. To cope with this problem but still find the intersections with a good performance a peak search in the binned legendre space is applied. For each bin the number of sinusoidal functions passing this area is counted. The bin with the highest weight is assumed to be the bin with the highest number of sinusoidal intersections. From the wire hits contributing to this bin a new track is created and the search is repeated with those hits deleted until a threshold in the bin entry is undercut. As the legendre space is mostly empty this procedure can be further improved in performance by refining the bin devision from very coarse bin sizes to finer ones only for those bins which have a certain amount of sinusoidal functions in them. Because these bins are divided into 4 subbins with every refinement step the concept is also called a quad tree search. The whole search is depicted in figure \ref{fig-quad-tree-search}.

After finding the possible hit subsets for the track candidates a post processing procedure consisting of hit reassignment, hit deletion and track merging is applied to account for energy losses, trajectories not coming from the interaction point and finding inefficiencies. This post processing is described in more detail further down.

\begin{figure}
  \centering
  \includegraphics[width=0.6\linewidth]{figures/theory/quad_tree.png}
  \caption{Depiction of a quad the search to find the bin with the most interceptions of sinograms shown in red here. Only the first round of the search without multicandidate search is shown for better visibility. Each color shows sinograms that belong to the same charged particle. Taken from \cite{viktor_dpg}.}
  \label{fig-quad-tree-search}
\end{figure}


\subsubsection{Stereo Hit Finding}

The before mentioned legendre track finding algorithm does only work for axial hits as a precise position in $x$ and $y$ is needed for calculating the position in the legendre space. Therefore it can not be applied for hits coming from stereo wires as they have a certain range of possible $x$-$y$-coordinates. But as soon as a trajectory with the axial hits is found, each stereo hit can be reconstructed to match the trajectory in such a way that its drift circle touches the trajectory circle of the candidate. As the stereo wire can be best approximated by a single line in 3d space the $z$ position is now also fixed. As it is not possible to gain information whether the drift circle is included in the trajectory circle or not - the same reason why there were two sinograms - each stereo hit leads to two slightly different reconstructed $z$ positions.

An ideal trajectory in the $z$-$r$ plane resembles a straight line analogous to the circle in the $r$-$\phi$ plane and can be described by two parameters also. This time these parameters are the slope\footnote{It is described by the angle $\lambda$ for consistency with the so called helix parameters} $\tan \lambda$ and the distance $z_0$ on the $z$-axis to the interaction point. The plane spanned by these two parameters is analogous to the legendre space in the axial case. As before each point corresponds to a certain trajectory. By using the two reconstructed $z$ information from each stereo hit two functions in this plane can be drawn which include every possible trajectory that would have passed the stereo hit. For axial hits these functions where given by the two sinograms. For stereo hits they are straight lines in the $\tan \lambda$-$z_0$ plane:
$$ z_0 = z_\text{rec} - \tan \lambda \cdot r_\text{rec} $$
with the reconstructed $z$ positions $z_\text{rec}$ and the reconstructed radii $r_\text{rec}$. The whole process can be seen in figure \ref{fig-stereo-explained}. Again, a quad tree search is applied to search for the point with the highest number of interceptions which corresponds to the trajectory parameters compatible with the largest number of hits. This time only the single highest trajectory is stored as one single trajectory in the $r$-$\phi$ plane can only have one single trajectory in the $r$-$z$ plane. The whole algorithm is repeated for all other found axial-only trajectories. 

As the energy loss has more or less no influence on the $z$ motion because of the much higher momentum in this direction and a non-zero distance to the interaction point is already accounted for, a post processing is not needed here. See the chapter on the SegmentTrackCombiner for a description how to cope with the remaining inefficiencies.


\begin{figure}
 \centering
 \includegraphics[scale=0.2]{figures/theory/stereo_1.png}
 \includegraphics[scale=0.2]{figures/theory/stereo_2.png}
 \includegraphics[scale=0.2]{figures/theory/stereo_3.png}
 \caption{Stereo hit finder algorithm shown for a single track. The track trajectory in the $r$-$\phi$ plane leads to only two possible $z$ positions for each stereo hit. On the left side the trajectory together with the stereo wires projected to the $r$-$\phi$ plane is show. In principle each stereo hit could have been created by an infinite number of tracks. Some of them are shown in color in the center picture. The dark blue track is the correct hypothesis. Each line correspond to a single point in the $\tan \lambda$-$z_0$ plane which is shown on the right side together with the correct trajectory parameters as a red diamond. The blue diamonds on the right side correspond to the blue lines in the center.}
 \label{fig-stereo-explained}
\end{figure}


\subsection{The Automaton Track Finder}
Before doing the quad tree search the legendre algorithm transforms each hit to the legendre space in the same manner making it a global hit finder. The automaton track finder however is a local track finder algorithm using the neighborhood relations among the hits. It processes the hit information in two stages:
\begin{zlist}
  \item Clusterize the hits into groups (so called clusters) and create segments out of these clusters. A segment is a smaller part of a track limited by the superlayer bounds. 
  \item Use these superlayer segments and combine them to track candidates.
\end{zlist}

Both steps include more or less the same problem: find the correct candidate among all combinations of items in a set. In the first step these items are the hit clusters, in the second step the segments. In both cases the problem is solved by applying a cellular automaton algorithm which is described briefly in the following. More information can for example be found in \cite{cats}, \cite{kisel} and \cite{oliver}. Each element in the set of items to be combined it modeled as a node in an acyclic directed graph. The edges in this graph represent the fact that a combination between the adjacent elements is in principle possible. The implementation of this decision is of course dependent on the item type and the purpose of the cellular automaton. The stated problems in tracking ensure that the graph is indeed cycleless and directed\footnote{A particle can not create the same CDC wire hit twice (It is directed because the natural flight direction of the tracks introduce a preferred direction and acyclic because a particle passing the same wire twice without leaving this superlayer is very rare.}. The cellular automaton now seeks to find the longest path in this directed graph - so the path with the highest number of passed nodes. To have a finer control on the output of the algorithm one can also apply weights to the edges and nodes of the graph. The output path is now not necessarily the longest path but the path with the highest sum of weights of the included edges and nodes. The cellular automaton algorithm finds this path by recursively traversing through the nodes and searching for the path with the highest sum of weights starting with this node. By passing this information to all parent nodes the output path can be easily found. If one is not only interested in the single best path this procedure is repeated with the already taken items removed from the graph.

The first step of the automaton track finder is to clusterize the hits into bunches. A cluster of hits is defined as the largest set of directly adjacent hits in a single superlayer. This condition can also be weakened to allow for one-cell-wide gaps in the clusters. The segments which are build next are fully contained in one single cluster - but one cluster can contain more than one segment when two tracks cross each other in this superlayer. To decide from which partition of hits in a cluster the segment should be build the cellular algorithm is applied for each cluster independently. The nodes in the graph are given by the so called facets. One facet is one of the possibilities of assembling three wire hits with two pairs of wire hits in close proximity to each other. Three possibilities for facets are shown in figure \ref{fig-facets}. As it can be seen in the figure each wire hit has already a right-left passage information attached to it and combinations of the same three wires with different right-left information form different facets. With this right-left information a direction of flight can be defined (except for the sign). Configurable filters are used to trim down the created three hit combinations to only compatible facets by limiting for example the angular deviation between the tangents to pairs of hits. Before applying the cellular automaton one has to define the edges of the graph also. Two facets have a directed connection in the graph if and only if the second and third hit of the first facet resembles the first and second hit of the second facet together with its right-left information and the angle between the defined directions of flight is belong a certain threshold. After performing the cellular automaton algorithm a set of paths including those facets is obtained. By collecting the hits included in those paths single segments can be build.

To construct the direction of flight and the tangents a distinct $x$-$y$ position for each hit is needed. For axial hits this position is given by the wire position but the stereo wires without knowing the $z$ position do not have fixed coordinates in this plane. The problem can be solved by using the positions of the stereo wires for $z = 0$ in the cellular algorithm. This does not produce a large error as the reciprocal position of the stereo hits in one layer does not change much with different $z$ positions.

In this thesis different possibilities to combine the results of the legendre track finder with the ones coming from the automaton track finder are described. For most of the cases the produced segments are used rather than full tracks for reasons described in later chapters. But to run the local track finder as a standalone algorithm the second step of creating track candidates out of the produced segments is also necessary. As this step is also performed using the cellular algorithm procedure the nodes and edges have to be defined again. One choice would be to use the segments as a natural choice for the nodes and connect each pair of segments in the graph directly if their distance is small enough. Another approach is to use pairs or even three segments laying in consecutive superlayers because these groups of segments have at least one stereo segment in them giving the track piece a $z$-information. This $z$-information can be used to create relations between those segment groups. Running the cellular automaton in a multipass manner leads to a collection of track candidates.


\section{The used Figures Of Merit}

For testing and developing and also for later usage in a physic analysis we need to compile numbers from the implemented track finder algorithms to see how well they work. There are three different classes of figures of merit to describe a track finder algorithm. All three classes listed here are described in more detail below. The three classes are:
\begin{itemize}
  \item the efficiency (like the hit efficiency or the finding efficiency, also split up for different particle types, momentum regions or areas in the detector)
  \item the error rate (like the clone or fake rate)
  \item the computational performance (like timing and memory consumption)
\end{itemize}

The last class should be quite clear and is measured by the basf2-own measuring algorithms. As the tracking is part of the online reconstruction and may be once adopted for the high level trigger, the timing performance is very important. 

The other two classes can best be described with the algorithm how they get computed. It starts with a full Monte Carlo simulation of generic BB events with the full detector simulation afterwards. The created hits can then be parted into distinct sets - each describing a simulated tracks, called MC track candidate or MCTrackCand\footnote{They are called track candidates also to fit the naming convention of the track finder although they are not candidates but rather the correct ones by definition.}. Only those tracks are kept as a MCTrackCand, that have at least 3 hits in the CDC - otherwise they can not be fitted and even if there were a chance to find them via track finding, they could not be used for physics. After that, the simulated hits with the stripped MC information are used for track finding with the method to be evaluated. This can be done easily as the simulated hits are transformed in a format that is similar to the format that will be used later for the data coming from the experiment. After the track finder has produced a list of track candidates also, we can use the saved MC information to match tracks from the track finder to tracks from the MC algorithm (also called MC track finder) by counting the number of hits they share. The different cases are depicted and described in table \ref{tab-mc-track-finder}.

\begin{table}
  \centering
  \begin{tabular}{m{0.4\linewidth}m{0.5\linewidth}} \toprule
    \centering \includegraphics[width=0.8\linewidth]{figures/theory/fom_found.pdf} & There is a one to one connection between a MCTrackCand and a track from the track finder. The MCTrackCand is labeled found and the other track is labeled matched. \\ \midrule
    \centering \includegraphics[width=0.8\linewidth]{figures/theory/fom_clone.pdf} & The MCTrackCand is found twice. The track from the track finder with the higher percentage (the green one in this example) is labeled matched, the other one cloned. The MCTrackCand is nevertheless labeled found. \\  \midrule
    \centering \includegraphics[width=0.8\linewidth]{figures/theory/fom_fake.pdf} & The track from the track finder is created with hits from many different MCTrackCands. As none of the corresponding hit ratios exceeds 66 \%, the track is called fake. There is no precise reason why the number 66\% was chosen. The hit ratios of the MCTrackCands itself do not play any role here. TODO: Is found for MC possible? \\  \midrule
    \centering \includegraphics[width=0.8\linewidth]{figures/theory/fom_background.pdf} & The found track does not describe any of the MCTrackCands well (or well enough) - but is made out of background hits. This track is also called a fake. \\ \bottomrule
  \end{tabular}
  \caption[Matching routine for compiling the FOM.]{This tabular shows the four different cases for the matching between tracks found by the track finder (on the left side of the pictures) and MCTrackCands (shown on the right side). The different colors differentiate between different tracks. The connection between tracks shows that these two tracks share hits. The two percentages on the arrows are the percentages of hits they share in respect to the total number of hits in the MCTrackCand/track candidate from the track finder.}
  \label{tab-mc-track-finder}
\end{table}

The finding efficiency now describes the rate of MCTrackCands which are labeled matched to the total amount of MC track candidates. Building this ratio can also be done for bins in various variables, like perpendicular momentum ($p_T$), angle in the curling plane ($phi$), number of tracks per event (multiplicity) and many more. A perfect track finder would have a finding efficiency of 100 \%. In most of the cases, the finding efficiency drops for tracks in a certain region of these variables - like low momentum tracks.
The hit efficiency is the mean of the ratios between the number of hits in the MCTrackCands matched to a non-fake track candidate to the number of hits in total of this track. A perfect track finder would have also a hit efficiency of 100 \%. In most of the cases the hit efficiency drops because of energy losses and deviations from the perfect trajectory form because of multiple scattering.
The fake and clone rates are the number of track candidates labeled as fake or clone by the matching algorithm divided by the number of found tracks in total. A perfect track finder would have both number set to 0 \%. A high fake rate is caused by a track finder with too loose cuts when putting together single pieces of tracks to a big track or by one which picks up background hits often. A track finder with a high clone rate on the other hand has too harsh cuts and splits up tracks into more than one piece often.

\section{Track Fitting}

The track finding algorithms have the task to partition the measured hits into sets with each set forming a single track candidate. After that the purpose of a track fitting algorithm is to fit a model for the trajectory to the measured information of the hits to gain the particles properties like the momentum or the vertex position. The model used for the fit can be very easy without taking into account material effects or energy loss. The track fitting algorithms implemented in \texttt{basf2} however try take care of the interaction of the particles with material correctly by using the same algorithms in calculating these material effects that are used for simulating the detector geometry. As the calculation itself depends strongly on the track parameters this implies recalculating the material effects of the particle with every fitting step. Therefore an algorithm like the Kalman filter with its iterative approach suits well to track fitting.

The Kalman fitter algorithm \cite{kalman} is based on the idea of iteratively adding measurements (in this case the positions of the hits associated with this track) to the current state of the trajectory. Therefore the parameters change with every newly added hit and should in principle converge to the correct trajectory parameters. The change in the parameters is calculated by extrapolating a model of the trajectory with the current parameters as nearly as possible to the position of the next hit measurement and comparing this extrapolated position with the real hit position. The deviation together with the errors of the measurement can be used to compile new parameters for the trajectory. By transversing back and forth several times through the whole hit set the final parameter estimation is found. One step in this procedure with only a small hit set is depicted in figure \ref{fig-kalman}. As described before the Kalman algorithm needs a current parameter state to calculate the extrapolation and the updated parameter set. As there is no possibility to gain a current parameter set before processing the first few hits the track finder must provide a starting seed for the track parameters.

\begin{figure}
 \centering
 \includegraphics[width=0.6\linewidth]{figures/theory/kalman.pdf}
 \caption{Sketch with one step in the kalman fitter procedure. The correct track parameters are depicted by the black trajectory. The green planes are given by two sensors. The correct track passage positions - the hit position measurements - are shown as black circles. The current state of the fitting parameters is shown in gray. The state is extrapolated from the lower to the upper plane (gray dashed) and a new reconstructed position is calculated (gray circle on the upper plane). As there is a deviation between current and correct hit position, the current trajectory is changed according to the blue arrow.}
 \label{fig-kalman}
\end{figure}

When taking into account fake or background hits in a track, the kalman fitter algorithm may not lead to good results anymore. As each hit is used for estimating the parameters of the trajectory a wrongly attached hit can give a wrong bias on these parameters. Therefore a weighting scheme is applied to all hits after each kalman passage and this weight defines how strong a hit effects the final parameters. There are many different ways to do so including elastic tracking and nonlinear filters \cite{daf_fruh}. The one currently used for the tracking in the Belle II software framework is the \emph{deterministic annealing filter} (DAF) introduced by FrÃ¼hwirth and Strandlie. The procedure is as follows:
\begin{zlist}
  \item Set the weights of all hits to 1.
  \item Fit the track with a Kalman fitter taking into account the weights of each hit. \label{daf-loop-start}
  \item Recalculate the weight of each hit with the distance of the hit to the current trajectory hypothesis.
  \item Dismiss hits with a weight below a certain threshold.
  \item Repeat with (\ref{daf-loop-start}) until a defined number of iterations is reached or the fit is converged.
\end{zlist}

To overcome the difficulties of badly chosen starting parameters of the fit that would lead to dismissing a large number of hits in the first iteration an annealing schema is applied: each weight is transformed with a Maxwell-Boltzmann distribution depending on a ``temperature'' $T$ which is decreased with every iteration. In the first few iterations with a high temperature hits with a small weight are kept while in later iterations these hits are dismissed.

The fitting procedures are implemented into \texttt{basf2} with the external library \texttt{genfit} \cite{genfit} - a library for generic and experiment independent track fitting. Only a small interface for accessing the \texttt{genfit} procedures from the \texttt{basf2} code is implemented. See the chapter on the VXD momentum estimation for more information on this topic.

\section{Multivariate Classification}

When doing track finding - especially in post processing procedures - many decisions must me made: whether a hit belongs to a track, whether two tracks should be merged, whether a track should be dismissed as fake etc. In the context of inefficiencies and fakes due to background hits these decisions may depend on many input variables. Additionally there is the need to have a smooth transition between the two corner cases ``dismiss all'' and ``accept all'' to allow for subsequent optimization. This problem of deciding between two different possibilities for each input element (for example each track) is called a classification problem in statistics and there are many ways to deal with such problems (see for example \cite{cowan} or \cite{blobel}). For most of the classification tasks in the track finding package for the CDC detector a classification algorithm known as \emph{boosted decision trees} (BDT) is used. In the following the main features are described. See \cite{keck} for more information on the implementation.

The task of a classification algorithm is to decide whether an input element described by a feature vector $\vec x$ belongs to one class of elements or the other - often these classes are called signal and background class. The classifier maps the multidimensional feature vector $\vec x$ to a one dimensional output variable - often between 0 and 1 - which is designed in a way to separate signal and background by mapping signal like data to values near 1 and background like data to values near 0. The output can therefore be interpreted as the probability for the input data to look like a signal sample and a configurable cut on the output can be used to distinguish between the two classes with a therefore also configurable purity and efficiency.

A decision tree is one possible implementation of such a classifier. A decision tree is created by dividing the parameter space of the feature vectors $\vec x$ into small hypercubes. As this is done by applying a division in one variable at the time a tree structure like in figure \ref{fig-decision-tree} is formed. Each input data sample can then by mapped to the signal-fraction of the hypercube it belongs to which is the BDT output for this element. This signal-fraction is determined with a training sample of monte carlo generated events with a known classification information. The cut values and variables are chosen in a way to maximize a separation measure like the gini impurity that models the gain introduced this separation. It is also calculated using the training sample. 

Because the chosen cuts depend strongly on the training sample a single decision tree can easily be overtrained. A classifier is called overtrained when its separation power on the testing data set is much better than on an independent testing sample because instead of generalizing the training data it fits its output function to the training data perfectly well. An overtrained classifier can onlye handle exactly those cases that were present in the training set. To avoid overtraining an algorithm known as boosting is applied to the decision tree. This algorithm creates a whole set of boosted decision trees iteratively. Each decision tree is trained to separate the items the one before had classified wrongly. It does so by applying weights to the items. The final output of the classifier is a weighted sum of all single trees. By limiting the depth of a single decision tree it avoids overtraining but because of the huge number of trees it still has a great separation power.

\begin{figure}
  \centering
  \begin{tikzpicture}[thick]
    \node[module] (alldata) {Decision Tree};
    \def\h{1}
    \def\x{0.8}
    \node[module, text width=6em, fill=kit-orange50, below=0.25 of alldata] (cut1) {$x_3 > a$?};
    \node[below=\h of cut1] (dummy1) {};
    \node[below=\h of dummy1] (dummy2) {};
    \node[below=\h of dummy2] (dummy3) {};
    \node[module, text width=6em, fill=kit-orange50, left=3*\x of dummy1] (cut2a) {$x_1 > b$?};
    \node[module, text width=6em,fill=kit-orange50, left=\x of dummy2] (cut3a) {$x_7 > d$?};
    \node[module, text width=6em,fill=kit-orange50, left=5*\x of dummy2] (cut3b) {$x_5 > e$?};
    \node[module, text width=6em,fill=kit-orange50, right=3*\x of dummy1] (cut2b) {$x_2 > f$?};
    \node[module, text width=6em,fill=kit-orange50, right=\x of dummy2] (cut3c) {$x_4 > g$?};
    \node[module, text width=6em,fill=kit-orange50, right=5*\x of dummy2] (cut3d) {$x_6 > h$?};
    
    \node[module, text width=6em, left=\x of dummy3] (result3a) {10 signal events};
    \node[module, text width=6em, left=5*\x of dummy3] (result3b) {328 signal events};
    \node[module, text width=6em, right=\x of dummy3] (result3c) {4136 signal events};
    \node[module, text width=6em, right=5*\x of dummy3] (result3d) {4 signal events};
    
    
   \draw[vecArrow] (cut1) -- (cut2a.north);
   \draw[vecArrow] (cut1) -- (cut2b.north);
   \draw[vecArrow] (cut2a) -- (cut3a.north);
   \draw[vecArrow] (cut2a) -- (cut3b.north);
   \draw[vecArrow] (cut2b) -- (cut3c.north);
   \draw[vecArrow] (cut2b) -- (cut3d.north);
\end{tikzpicture}
\caption{A fictitious decision tree dividing a 7-dimensional parameter space in 4 hypercubes with three layers of seven cuts. The last layer is used to generate the output of the decision tree by counting the number of signal events in this hypercube in the training sample.}
\label{fig-decision-tree}
\end{figure}


